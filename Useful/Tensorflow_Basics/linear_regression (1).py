# -*- coding: utf-8 -*-
"""Linear_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fiGkgv-v4RaK7lVJSYCR2L_yMpiZMCXN
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

#y = mx +b

x = tf.placeholder(dtype = tf.float32)
y = tf.placeholder(dtype = tf.float32)
m = tf.Variable([-0.5], dtype = np.float32)
b = tf.Variable([0.5], dtype = np.float32)

lin = m*x + b

loss = tf.reduce_sum(tf.square(lin-y)) #loss function 
#reducing sum of squared errors between the model and the real values
optimizer = tf.train.GradientDescentOptimizer(0.01)  #learning rate of 0.01
train = optimizer.minimize(loss)
x_train = [1,2,3,4]
#1st try : [ 0.  -0.5 -1.  -1.5]
y_train = [0,-1,-2,-3]
session = tf.Session()
init = tf.global_variables_initializer()
session.run(init)

#print(session.run(lin ,{x: x_train}))

for i in range(1000):   #basically number of epochs
   session.run(train, {x: x_train, y: y_train})
    
m, b, l = session.run([m[0],b[0], loss], {x: x_train, y: y_train})
print([m,b,l])

plt.scatter(x_train, y_train,color = 'red',marker = 'x')
x = np.linspace(1,4)
y = m* x + b
plt.plot(x,y,color = 'green')